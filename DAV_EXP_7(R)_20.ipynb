{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swayamg21/DAV_Experiments_17/blob/main/DAV_EXP_7_17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd8YA9c3m_H0"
      },
      "source": [
        "# **Aim : Perform the steps involved in Text Analytics in Python & R**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YoOcs1xnV7L"
      },
      "source": [
        "# **Text Analytics in Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_vcdZpandQY"
      },
      "source": [
        "**1. Tokenization (Sentence & Word)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8T9bVbAm7nK",
        "outputId": "d15cc12d-2eed-4516-f8fd-ee7cc6601c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences: ['This is a sample sentence.', 'Tokenization is important for NLP.']\n",
            "Words: ['This', 'is', 'a', 'sample', 'sentence', '.', 'Tokenization', 'is', 'important', 'for', 'NLP', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncM66LEqn49j"
      },
      "source": [
        "**2. Frequency Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuTu4Of1vZNp",
        "outputId": "0da2e48e-ab1d-4454-e661-42ee5002dc45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is: 2\n",
            ".: 2\n",
            "This: 1\n",
            "a: 1\n",
            "sample: 1\n"
          ]
        }
      ],
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Create a frequency distribution of the words in the text\n",
        "freq_dist = FreqDist(words)\n",
        "\n",
        "# Print the most common words and their frequencies\n",
        "for word, frequency in freq_dist.most_common(5):\n",
        "    print(f\"{word}: {frequency}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OE9Evtan2Z-",
        "outputId": "faddc23b-cb41-4534-d20a-d0b9bf975bf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word frequency: <FreqDist with 10 samples and 12 outcomes>\n"
          ]
        }
      ],
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "word_freq = FreqDist(words)\n",
        "print(\"Word frequency:\", word_freq)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA2dhUeXocbE"
      },
      "source": [
        "**3. Remove stopwords & punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2yXofSkoYlI",
        "outputId": "f029a764-5963-44db-f687-259edb4422c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['sample', 'sentence', 'Tokenization', 'important', 'NLP']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Remove punctuations\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "words = [word for word in words if word not in punctuations]\n",
        "\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQbIh5nopBLq"
      },
      "source": [
        "**4. Lexicon Normalization (Stemming, Lemmatization)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80DA5XBCph_u",
        "outputId": "2e265ee4-c8bc-4909-9409-66c5c523f7b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemmed words: ['thi', 'is', 'a', 'sampl', 'sentenc', '.', 'token', 'is', 'import', 'for', 'nlp', '.']\n",
            "Lemmatized words: ['This', 'is', 'a', 'sample', 'sentence', '.', 'Tokenization', 'is', 'important', 'for', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "stemmed_words = [ps.stem(word) for word in words]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Stemmed words:\", stemmed_words)\n",
        "print(\"Lemmatized words:\", lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCt3MVI1p_KK"
      },
      "source": [
        "**5. Part of Speech tagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH1NN8ETqN3O",
        "outputId": "8eaaa6f1-3a94-41f9-b8bf-9abbd9b45f33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Part Of Speech Tagging: [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('sample', 'JJ'), ('sentence', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "tagged_words = nltk.pos_tag(words)\n",
        "print(\"Part Of Speech Tagging:\", tagged_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ElNFDPqeIp"
      },
      "source": [
        "**6. Named Entity Recognization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYBy5m1Lqdwa",
        "outputId": "65ef0185-a36f-432b-be81-566a15688d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Barack/NNP)\n",
            "  (PERSON Obama/NNP)\n",
            "  was/VBD\n",
            "  born/VBN\n",
            "  in/IN\n",
            "  (GPE Honolulu/NNP)\n",
            "  ,/,\n",
            "  (GPE Hawaii/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "\n",
        "text = \"Barack Obama was born in Honolulu, Hawaii.\"\n",
        "words = word_tokenize(text)\n",
        "tagged_words = nltk.pos_tag(words)\n",
        "\n",
        "ne_tree = nltk.ne_chunk(tagged_words)\n",
        "print(ne_tree)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXduEXItq70_"
      },
      "source": [
        "**7. Scrape data from a website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa0LipRnrFB8",
        "outputId": "8cb22739-e95e-44ee-d3e1-07cfa7511c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text data from website: \n",
            "\n",
            "\n",
            "Example Domain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Example Domain\n",
            "This domain is for use in illustrative examples in documents. You may use this\n",
            "    domain in literature without prior coordination or asking for permission.\n",
            "More information...\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = 'https://example.com'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "text_data = soup.get_text()\n",
        "print(\"Text data from website:\", text_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GqUrN2lsVWK"
      },
      "source": [
        "# **Text Analytics in R**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EYnSpp2sZOR"
      },
      "source": [
        "**1. Tokenization (Sentence & Word)**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization (Sentence & Word)\n",
        "text <- \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences <- strsplit(text, \"\\\\.\")[[1]]\n",
        "words <- unlist(strsplit(text, \"\\\\s+\"))\n",
        "\n",
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "print(\"Words:\")\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoC9nJigDZt9",
        "outputId": "78c2251a-2af6-44ef-8494-b20eca6265fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentences:\"\n",
            "[1] \"This is a sample sentence\"          \" Tokenization is important for NLP\"\n",
            "[1] \"Words:\"\n",
            " [1] \"This\"         \"is\"           \"a\"            \"sample\"       \"sentence.\"   \n",
            " [6] \"Tokenization\" \"is\"           \"important\"    \"for\"          \"NLP.\"        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMK6zkKvscTW",
        "outputId": "fa74e764-4d9b-4cd6-fbce-a34f7aac2592"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘Rcpp’, ‘SnowballC’\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Sentences:\"\n",
            "[[1]]\n",
            "[1] \"This is a sample sentence.\"         \"Tokenization is important for NLP.\"\n",
            "\n",
            "[1] \"Words:\"\n",
            "[[1]]\n",
            " [1] \"this\"         \"is\"           \"a\"            \"sample\"       \"sentence\"    \n",
            " [6] \"tokenization\" \"is\"           \"important\"    \"for\"          \"nlp\"         \n",
            "\n"
          ]
        }
      ],
      "source": [
        "install.packages(\"tokenizers\")\n",
        "library(tokenizers)\n",
        "\n",
        "text <- \"This is a sample sentence. Tokenization is important for NLP.\"\n",
        "sentences <- tokenize_sentences(text)\n",
        "words <- tokenize_words(text)\n",
        "\n",
        "print(\"Sentences:\")\n",
        "print(sentences)\n",
        "print(\"Words:\")\n",
        "print(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytwKTFo2suc4"
      },
      "source": [
        "**2. Frequency Distribution**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency Distribution\n",
        "word_freq <- table(words)\n",
        "print(\"Word frequency:\")\n",
        "print(word_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjjNOQNyDnTc",
        "outputId": "5d81fee9-c489-44af-fdbf-a6137fdfd353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Word frequency:\"\n",
            "words\n",
            "           a          for    important           is         NLP.       sample \n",
            "           1            1            1            2            1            1 \n",
            "   sentence.         This Tokenization \n",
            "           1            1            1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OwmHXrZsuZU"
      },
      "source": [
        "**3. Remove stopwords & punctuations**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords & punctuations\n",
        "stop_words <- c(\"is\", \"a\", \"for\")  # Example list of stopwords\n",
        "filtered_words <- words[!tolower(words) %in% stop_words & !grepl(\"[[:punct:]]\", words)]\n",
        "print(\"Filtered words:\")\n",
        " print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29vIl9mVDq5F",
        "outputId": "f511bf33-71c9-4ee8-cc4a-359359789c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Filtered words:\"\n",
            "[1] \"This\"         \"sample\"       \"Tokenization\" \"important\"   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OjLYNhcsuW8"
      },
      "source": [
        "**4. Lexicon Normalization (Stemming, Lemmatization)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexicon Normalization (Stemming, Lemmatization)\n",
        "# For Stemming\n",
        "stemmed_words <- wordStem(filtered_words, language = \"en\")\n",
        "\n",
        "# For Lemmatization (using example)\n",
        "install.packages(\"udpipe\")\n",
        "library(udpipe)\n",
        "ud_model <- udpipe_download_model(language = \"english\")\n",
        "ud_model <- udpipe_load_model(ud_model$file_model)\n",
        "x <- udpipe_annotate(ud_model, texts = filtered_words, doc_id = 1:length(filtered_words))\n",
        "lemmatized_words <- as.data.frame(x)$lemma\n",
        "\n",
        "print(\"Stemmed words:\")\n",
        "print(stemmed_words)\n",
        "print(\"Lemmatized words:\")\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "id": "y2QKJihvD8SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_pzDNqjsuUh"
      },
      "source": [
        "**5. Part of Speech tagging**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of Speech tagging\n",
        "# Using udpipe library\n",
        "x <- udpipe_annotate(ud_model, texts = filtered_words, doc_id = 1:length(filtered_words))\n",
        "pos_tags <- as.data.frame(x)$upos\n",
        "\n",
        "print(\"POS tagging:\")\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "Q0w4raFNEZ4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLCRx7sOsuRm"
      },
      "source": [
        "**6. Named Entity Recognization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12PH6X3ztgE2"
      },
      "outputs": [],
      "source": [
        "install.packages(\"NLP\")\n",
        "install.packages(\"openNLP\")\n",
        "library(openNLP)\n",
        "library(NLP)\n",
        "\n",
        "ner_tags <- maxent_tagger_chunker(filtered_words, pos_tags)\n",
        "print(\"Named Entities:\")\n",
        "print( ner_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oUPp2QcsuIl"
      },
      "source": [
        "**7. Scrape data from a website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ECqdvU-thCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff6d963a-d28b-4706-ff1d-45aaa58de3c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Text data from website:\"\n",
            "[1] \"Example Domain\\n    body {\\n        background-color: #f0f0f2;\\n        margin: 0;\\n        padding: 0;\\n        font-family: -apple-system, system-ui, BlinkMacSystemFont, \\\"Segoe UI\\\", \\\"Open Sans\\\", \\\"Helvetica Neue\\\", Helvetica, Arial, sans-serif;\\n        \\n    }\\n    div {\\n        width: 600px;\\n        margin: 5em auto;\\n        padding: 2em;\\n        background-color: #fdfdff;\\n        border-radius: 0.5em;\\n        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\\n    }\\n    a:link, a:visited {\\n        color: #38488f;\\n        text-decoration: none;\\n    }\\n    @media (max-width: 700px) {\\n        div {\\n            margin: 0 auto;\\n            width: auto;\\n        }\\n    }\\n    \\n\\n    Example Domain\\n    This domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\n    More information...\\n\\n\"\n"
          ]
        }
      ],
      "source": [
        "install.packages(\"rvest\")\n",
        "library(rvest)\n",
        "\n",
        "url <- \"https://example.com\"\n",
        "page <- read_html(url)\n",
        "text_data <- page %>%\n",
        "  html_text()\n",
        "\n",
        "print(\"Text data from website:\")\n",
        "print(text_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmMRreRZstxD"
      },
      "source": [
        "# **Outcome :**\n",
        "1. Identified the Text Analytics Libraries in Python and R\n",
        "2. Performed simple experiments with these libraries in Python and R"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT3/Tfwt8OYlnDvtXxF6ui",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}